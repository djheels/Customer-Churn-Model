{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer Churn\n",
    "__Author__: David O'Donnell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement and Summary "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description of the problem and translation of business problem to machine learning problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data and Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set seaborn as default template for plots\n",
    "sns.set()\n",
    "\n",
    "# Expand pandas display output\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset from CSV\n",
    "path = '/Users/davidodonnell/Documents/Under Armour Working Sample/'\n",
    "filename = 'train.csv'\n",
    "\n",
    "df = pd.read_csv(path+filename, header=0, parse_dates=True).rename(columns={'apparell_spend': 'apparel_spend'})"
   ]
  },
  {
   "source": [
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## View Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 2666 entries, 0 to 2665\nData columns (total 18 columns):\n #   Column           Non-Null Count  Dtype  \n---  ------           --------------  -----  \n 0   last_purchase    2666 non-null   float64\n 1   max_discount     2665 non-null   float64\n 2   shoe_spend       2666 non-null   float64\n 3   apparel_spend    2666 non-null   object \n 4   acc_spend        2666 non-null   int64  \n 5   custserv_calls   2666 non-null   int64  \n 6   churn            2666 non-null   int64  \n 7   acc_purchasers   2666 non-null   int64  \n 8   promo_purchaser  2666 non-null   int64  \n 9   shoe_orders      2666 non-null   int64  \n 10  apparel_orders   2666 non-null   int64  \n 11  acc_orders       2666 non-null   int64  \n 12  gender           2666 non-null   object \n 13  ecommShopper     2666 non-null   bool   \n 14  bhShopper        2666 non-null   bool   \n 15  state            2666 non-null   object \n 16  area_code        2666 non-null   int64  \n 17  phone            2666 non-null   object \ndtypes: bool(2), float64(3), int64(9), object(4)\nmemory usage: 899.2 KB\n"
     ]
    }
   ],
   "source": [
    "# View Columns and Memory Usage\n",
    "df.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   last_purchase  max_discount  shoe_spend apparel_spend  acc_spend  \\\n",
       "0           56.5          0.26       322.2         194.3        126   \n",
       "1           84.0          0.46       279.1         170.9         92   \n",
       "2           96.0          0.00       294.7           306         96   \n",
       "3           62.0          0.00       255.4         185.6        100   \n",
       "4           45.0          0.28       300.6         197.9        154   \n",
       "5           68.5          0.00       243.4           217         96   \n",
       "6           58.0          0.00       241.9         137.9         77   \n",
       "7           62.0          0.26       226.3         220.3         66   \n",
       "8          101.5          0.00       180.7          97.8         93   \n",
       "9           80.0          0.32       159.7         124.4        106   \n",
       "\n",
       "   custserv_calls  churn  acc_purchasers  promo_purchaser  shoe_orders  \\\n",
       "0               1      0               0                1            3   \n",
       "1               0      0               0                1            2   \n",
       "2               1      1               0                0            2   \n",
       "3               2      0               0                0            2   \n",
       "4               0      0               0                1            3   \n",
       "5               0      0               0                0            2   \n",
       "6               1      1               1                0            2   \n",
       "7               1      0               0                1            2   \n",
       "8               1      0               0                0            1   \n",
       "9               2      0               1                1            1   \n",
       "\n",
       "   apparel_orders  acc_orders  gender  ecommShopper  bhShopper state  \\\n",
       "0               2           4    Male         False       True    MS   \n",
       "1               2           3    Male         False      False    OH   \n",
       "2               3           3  Female          True      False    MI   \n",
       "3               2           3    Male         False      False    VT   \n",
       "4               2           5    Male         False       True    WV   \n",
       "5               3           3    Male          True      False    FL   \n",
       "6               2           3  Female         False       True    MA   \n",
       "7               3           2  Female         False      False    WA   \n",
       "8               2           3    Male         False       True    NY   \n",
       "9               2           3  Female         False       True    ID   \n",
       "\n",
       "   area_code     phone  \n",
       "0        510  402-5509  \n",
       "1        510  370-3021  \n",
       "2        415  373-1448  \n",
       "3        510  403-1769  \n",
       "4        408  405-9384  \n",
       "5        408  384-6654  \n",
       "6        408  371-9457  \n",
       "7        415  380-6631  \n",
       "8        510  379-2991  \n",
       "9        415  345-5980  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>last_purchase</th>\n      <th>max_discount</th>\n      <th>shoe_spend</th>\n      <th>apparel_spend</th>\n      <th>acc_spend</th>\n      <th>custserv_calls</th>\n      <th>churn</th>\n      <th>acc_purchasers</th>\n      <th>promo_purchaser</th>\n      <th>shoe_orders</th>\n      <th>apparel_orders</th>\n      <th>acc_orders</th>\n      <th>gender</th>\n      <th>ecommShopper</th>\n      <th>bhShopper</th>\n      <th>state</th>\n      <th>area_code</th>\n      <th>phone</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>56.5</td>\n      <td>0.26</td>\n      <td>322.2</td>\n      <td>194.3</td>\n      <td>126</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>3</td>\n      <td>2</td>\n      <td>4</td>\n      <td>Male</td>\n      <td>False</td>\n      <td>True</td>\n      <td>MS</td>\n      <td>510</td>\n      <td>402-5509</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>84.0</td>\n      <td>0.46</td>\n      <td>279.1</td>\n      <td>170.9</td>\n      <td>92</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>2</td>\n      <td>2</td>\n      <td>3</td>\n      <td>Male</td>\n      <td>False</td>\n      <td>False</td>\n      <td>OH</td>\n      <td>510</td>\n      <td>370-3021</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>96.0</td>\n      <td>0.00</td>\n      <td>294.7</td>\n      <td>306</td>\n      <td>96</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>3</td>\n      <td>3</td>\n      <td>Female</td>\n      <td>True</td>\n      <td>False</td>\n      <td>MI</td>\n      <td>415</td>\n      <td>373-1448</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>62.0</td>\n      <td>0.00</td>\n      <td>255.4</td>\n      <td>185.6</td>\n      <td>100</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>2</td>\n      <td>3</td>\n      <td>Male</td>\n      <td>False</td>\n      <td>False</td>\n      <td>VT</td>\n      <td>510</td>\n      <td>403-1769</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>45.0</td>\n      <td>0.28</td>\n      <td>300.6</td>\n      <td>197.9</td>\n      <td>154</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>3</td>\n      <td>2</td>\n      <td>5</td>\n      <td>Male</td>\n      <td>False</td>\n      <td>True</td>\n      <td>WV</td>\n      <td>408</td>\n      <td>405-9384</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>68.5</td>\n      <td>0.00</td>\n      <td>243.4</td>\n      <td>217</td>\n      <td>96</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>3</td>\n      <td>3</td>\n      <td>Male</td>\n      <td>True</td>\n      <td>False</td>\n      <td>FL</td>\n      <td>408</td>\n      <td>384-6654</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>58.0</td>\n      <td>0.00</td>\n      <td>241.9</td>\n      <td>137.9</td>\n      <td>77</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>2</td>\n      <td>2</td>\n      <td>3</td>\n      <td>Female</td>\n      <td>False</td>\n      <td>True</td>\n      <td>MA</td>\n      <td>408</td>\n      <td>371-9457</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>62.0</td>\n      <td>0.26</td>\n      <td>226.3</td>\n      <td>220.3</td>\n      <td>66</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>2</td>\n      <td>3</td>\n      <td>2</td>\n      <td>Female</td>\n      <td>False</td>\n      <td>False</td>\n      <td>WA</td>\n      <td>415</td>\n      <td>380-6631</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>101.5</td>\n      <td>0.00</td>\n      <td>180.7</td>\n      <td>97.8</td>\n      <td>93</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>2</td>\n      <td>3</td>\n      <td>Male</td>\n      <td>False</td>\n      <td>True</td>\n      <td>NY</td>\n      <td>510</td>\n      <td>379-2991</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>80.0</td>\n      <td>0.32</td>\n      <td>159.7</td>\n      <td>124.4</td>\n      <td>106</td>\n      <td>2</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>3</td>\n      <td>Female</td>\n      <td>False</td>\n      <td>True</td>\n      <td>ID</td>\n      <td>415</td>\n      <td>345-5980</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 94
    }
   ],
   "source": [
    "# View First Ten Lines of Data\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "       last_purchase  max_discount   shoe_spend    acc_spend  custserv_calls  \\\n",
       "count    2666.000000   2665.000000  2666.000000  2666.000000     2666.000000   \n",
       "mean       70.961553      0.081418   245.565304   102.430983        1.556264   \n",
       "std        19.978791      0.136785    54.538190    27.921276        1.312955   \n",
       "min         0.000000      0.000000    65.000000     0.000000        0.000000   \n",
       "25%        57.625000      0.000000   208.500000    85.000000        1.000000   \n",
       "50%        71.500000      0.000000   245.350000   103.000000        1.000000   \n",
       "75%        84.500000      0.200000   282.675000   121.000000        2.000000   \n",
       "max       121.000000      0.500000   411.800000   200.000000        9.000000   \n",
       "\n",
       "             churn  acc_purchasers  promo_purchaser  shoe_orders  \\\n",
       "count  2666.000000     2666.000000      2666.000000  2666.000000   \n",
       "mean      0.149287        0.097524         0.277944     2.044261   \n",
       "std       0.356438        0.296726         0.448070     0.534996   \n",
       "min       0.000000        0.000000         0.000000     1.000000   \n",
       "25%       0.000000        0.000000         0.000000     2.000000   \n",
       "50%       0.000000        0.000000         0.000000     2.000000   \n",
       "75%       0.000000        0.000000         1.000000     2.000000   \n",
       "max       1.000000        1.000000         1.000000     3.000000   \n",
       "\n",
       "       apparel_orders   acc_orders    area_code  \n",
       "count     2666.000000  2666.000000  2666.000000  \n",
       "mean         2.159790     3.251688   437.737059  \n",
       "std          0.495355     0.816830    42.679743  \n",
       "min          1.000000     0.000000   408.000000  \n",
       "25%          2.000000     3.000000   415.000000  \n",
       "50%          2.000000     3.000000   415.000000  \n",
       "75%          2.000000     4.000000   510.000000  \n",
       "max          3.000000     6.000000   510.000000  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>last_purchase</th>\n      <th>max_discount</th>\n      <th>shoe_spend</th>\n      <th>acc_spend</th>\n      <th>custserv_calls</th>\n      <th>churn</th>\n      <th>acc_purchasers</th>\n      <th>promo_purchaser</th>\n      <th>shoe_orders</th>\n      <th>apparel_orders</th>\n      <th>acc_orders</th>\n      <th>area_code</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>2666.000000</td>\n      <td>2665.000000</td>\n      <td>2666.000000</td>\n      <td>2666.000000</td>\n      <td>2666.000000</td>\n      <td>2666.000000</td>\n      <td>2666.000000</td>\n      <td>2666.000000</td>\n      <td>2666.000000</td>\n      <td>2666.000000</td>\n      <td>2666.000000</td>\n      <td>2666.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>70.961553</td>\n      <td>0.081418</td>\n      <td>245.565304</td>\n      <td>102.430983</td>\n      <td>1.556264</td>\n      <td>0.149287</td>\n      <td>0.097524</td>\n      <td>0.277944</td>\n      <td>2.044261</td>\n      <td>2.159790</td>\n      <td>3.251688</td>\n      <td>437.737059</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>19.978791</td>\n      <td>0.136785</td>\n      <td>54.538190</td>\n      <td>27.921276</td>\n      <td>1.312955</td>\n      <td>0.356438</td>\n      <td>0.296726</td>\n      <td>0.448070</td>\n      <td>0.534996</td>\n      <td>0.495355</td>\n      <td>0.816830</td>\n      <td>42.679743</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>65.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>408.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>57.625000</td>\n      <td>0.000000</td>\n      <td>208.500000</td>\n      <td>85.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>2.000000</td>\n      <td>2.000000</td>\n      <td>3.000000</td>\n      <td>415.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>71.500000</td>\n      <td>0.000000</td>\n      <td>245.350000</td>\n      <td>103.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>2.000000</td>\n      <td>2.000000</td>\n      <td>3.000000</td>\n      <td>415.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>84.500000</td>\n      <td>0.200000</td>\n      <td>282.675000</td>\n      <td>121.000000</td>\n      <td>2.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>2.000000</td>\n      <td>2.000000</td>\n      <td>4.000000</td>\n      <td>510.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>121.000000</td>\n      <td>0.500000</td>\n      <td>411.800000</td>\n      <td>200.000000</td>\n      <td>9.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>3.000000</td>\n      <td>3.000000</td>\n      <td>6.000000</td>\n      <td>510.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 95
    }
   ],
   "source": [
    "# View Numerical Data\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "last_purchase: 210\nmax_discount: 44\nshoe_spend: 1507\napparel_spend: 1447\nacc_spend: 160\ncustserv_calls: 10\nchurn: 2\nacc_purchasers: 2\npromo_purchaser: 2\nshoe_orders: 3\napparel_orders: 3\nacc_orders: 7\ngender: 2\necommShopper: 2\nbhShopper: 2\nstate: 53\narea_code: 3\nphone: 2666\n"
     ]
    }
   ],
   "source": [
    "# Number of Unique Values per Column\n",
    "for col in df.columns:\n",
    "    print(F'{col}: {df[col].nunique()}')"
   ]
  },
  {
   "source": [
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleanse Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Up Data (Types and Errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improper value in apparell_spend\n",
    "df.loc[df['apparel_spend']=='a']\n",
    "\n",
    "# Impute errant value with median apparell_spend/apparel_orders * apparel_orders\n",
    "median_apparel_spend_order = df[['apparel_spend']].loc[df['apparel_spend']!='a'].median().values//df[['apparel_orders']].loc[df['apparel_spend']!='a'].median().values\n",
    "\n",
    "df['apparel_spend'] = np.where(df['apparel_spend']!='a', df['apparel_spend'], median_apparell_spend_order*df['apparel_orders'])"
   ]
  },
  {
   "source": [
    "### Adjust Column Datatypes"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format Continuous Variables as Floats\n",
    "float_cols = [\n",
    "    'last_purchase',\n",
    "    'max_discount',\n",
    "    'shoe_spend',\n",
    "    'apparel_spend',\n",
    "    'acc_spend'\n",
    "]\n",
    "\n",
    "for col in float_cols:\n",
    "    df[col] = df[col].astype(str).str.replace(',', '')\n",
    "    df[col] = df[col].astype(str).str.replace('$', '')\n",
    "    df[col] = df[col].astype(float, errors='ignore')\n",
    "\n",
    "\n",
    "# Format Continuous Variables as Floats\n",
    "int_cols = [\n",
    "    'custserv_calls',\n",
    "    'shoe_orders',\n",
    "    'apparel_orders',\n",
    "    'acc_orders'\n",
    "]\n",
    "\n",
    "for col in int_cols:\n",
    "    df[col] = df[col].astype(str).str.replace(',', '')\n",
    "    df[col] = df[col].astype(str).str.replace('$', '')\n",
    "    df[col] = df[col].astype(float).round(0).astype(int)\n",
    "\n",
    "\n",
    "# Format Continuous Variables as Floats\n",
    "bool_cols = [\n",
    "    'churn',\n",
    "    'acc_purchasers',\n",
    "    'promo_purchaser',\n",
    "    'ecommShopper',\n",
    "    'bhShopper'\n",
    "]\n",
    "\n",
    "for col in bool_cols:\n",
    "    df[col] = df[col].astype(bool)\n",
    "\n",
    "# Format Categorical Variables\n",
    "ex_cols = [] # Other columns that shouldn't be converted to a specific datatype\n",
    "exclude_columns = float_cols+int_cols+bool_cols+ex_cols\n",
    "\n",
    "\n",
    "# Categorical Data Types\n",
    "n = 100 # Maximum number of unique values in a column to be classified as \"category\"\n",
    "\n",
    "for col in df.columns:\n",
    "    if df[col].nunique() < n and col not in exclude_columns:\n",
    "        df[col] = df[col].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 2666 entries, 0 to 2665\nData columns (total 18 columns):\n #   Column           Non-Null Count  Dtype   \n---  ------           --------------  -----   \n 0   last_purchase    2666 non-null   float64 \n 1   max_discount     2665 non-null   float64 \n 2   shoe_spend       2666 non-null   float64 \n 3   apparel_spend    2666 non-null   float64 \n 4   acc_spend        2666 non-null   float64 \n 5   custserv_calls   2666 non-null   int64   \n 6   churn            2666 non-null   bool    \n 7   acc_purchasers   2666 non-null   bool    \n 8   promo_purchaser  2666 non-null   bool    \n 9   shoe_orders      2666 non-null   int64   \n 10  apparel_orders   2666 non-null   int64   \n 11  acc_orders       2666 non-null   int64   \n 12  gender           2666 non-null   category\n 13  ecommShopper     2666 non-null   bool    \n 14  bhShopper        2666 non-null   bool    \n 15  state            2666 non-null   category\n 16  area_code        2666 non-null   category\n 17  phone            2666 non-null   object  \ndtypes: bool(5), category(3), float64(5), int64(4), object(1)\nmemory usage: 383.5 KB\n"
     ]
    }
   ],
   "source": [
    "df.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency Table\n",
    "### Description:\n",
    "* Insert description here\n",
    "\n",
    "### Use Cases:\n",
    "* Use Case #1, Use Case #2, etc.\n",
    "\n",
    "### Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pd.crosstab(index=df['col'], columns=df['col'], margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Data Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Factor Plot\n",
    "### Description:\n",
    "* Insert description here\n",
    "\n",
    "### Use Cases:\n",
    "* Use Case #1, Use Case #2, etc.\n",
    "\n",
    "### Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.factorplot(x='col', y='col', data=df, kind='plot type')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Plot\n",
    "### Description:\n",
    "* Insert description here\n",
    "\n",
    "### Use Cases:\n",
    "* Use Case #1, Use Case #2, etc.\n",
    "\n",
    "### Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='col', data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bar Plot\n",
    "### Description:\n",
    "* Insert description here\n",
    "\n",
    "### Use Cases:\n",
    "* Use Case #1, Use Case #2, etc.\n",
    "\n",
    "### Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x='col', y='col', data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Box Plot\n",
    "### Description:\n",
    "* Insert description here\n",
    "\n",
    "### Use Cases:\n",
    "* Use Case #1, Use Case #2, etc.\n",
    "\n",
    "### Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example\n",
    "sns.boxplot(x='col', y='col', data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Violin Plot\n",
    "### Description:\n",
    "* Insert description here\n",
    "\n",
    "### Use Cases:\n",
    "* Use Case #1, Use Case #2, etc.\n",
    "\n",
    "### Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.violinplot(x='col', y='col', data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strip Plot\n",
    "### Description:\n",
    "* Insert description here\n",
    "\n",
    "### Use Cases:\n",
    "* Use Case #1, Use Case #2, etc.\n",
    "\n",
    "### Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.stripplot(x='col', y='col', data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Swarm Plot\n",
    "### Description:\n",
    "* Insert description here\n",
    "\n",
    "### Use Cases:\n",
    "* Use Case #1, Use Case #2, etc.\n",
    "\n",
    "### Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.swarmplot(x='col', y='col', data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution Plot/Histogram\n",
    "### Description:\n",
    "* Insert description here\n",
    "\n",
    "### Use Cases:\n",
    "* Use Case #1, Use Case #2, etc.\n",
    "\n",
    "### Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df['col'], kde=False, color='red', bins=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join Plot\n",
    "### Description:\n",
    "* Insert description here\n",
    "\n",
    "### Use Cases:\n",
    "* Use Case #1, Use Case #2, etc.\n",
    "\n",
    "### Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.joinplot(x='col', y='col', data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Density Plot\n",
    "### Description:\n",
    "* Insert description here\n",
    "\n",
    "### Use Cases:\n",
    "* Use Case #1, Use Case #2, etc.\n",
    "\n",
    "### Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(df['col'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pair Plot\n",
    "### Description:\n",
    "* Insert description here\n",
    "\n",
    "### Use Cases:\n",
    "* Use Case #1, Use Case #2, etc.\n",
    "\n",
    "### Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df, hue='col')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rug Plot\n",
    "### Description:\n",
    "* Insert description here\n",
    "\n",
    "### Use Cases:\n",
    "* Use Case #1, Use Case #2, etc.\n",
    "\n",
    "### Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.rugplot(df['col'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relational Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relational Plots\n",
    "### Description:\n",
    "* Insert description here\n",
    "\n",
    "### Use Cases:\n",
    "* Use Case #1, Use Case #2, etc.\n",
    "\n",
    "### Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(x='col', y='col', data=df, kind='plot type') #default plot type is a scatterplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scatterplot\n",
    "### Description:\n",
    "* Insert description here\n",
    "\n",
    "### Use Cases:\n",
    "* Use Case #1, Use Case #2, etc.\n",
    "\n",
    "### Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x='col', y='col', data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Plot\n",
    "### Description:\n",
    "* Insert description here\n",
    "\n",
    "### Use Cases:\n",
    "* Use Case #1, Use Case #2, etc.\n",
    "\n",
    "### Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(x='col', y='col', data=df, hue='col')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Matrix\n",
    "### Description:\n",
    "* Insert description here\n",
    "\n",
    "### Use Cases:\n",
    "* Use Case #1, Use Case #2, etc.\n",
    "\n",
    "### Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df.corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Line Plot\n",
    "### Description:\n",
    "* Insert description here\n",
    "\n",
    "### Use Cases:\n",
    "* Use Case #1, Use Case #2, etc.\n",
    "\n",
    "### Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x='col', y='col', data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Manipulation and Additional Cleaning"
   ]
  },
  {
   "source": [
    "### Handling Missing Data and Null Values"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine Missing and Null Values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Observations with Null Values (based on Column/Column Type)\n",
    "df = df.dropna(how='all')\n",
    "df = df.dropna(subset=[col])"
   ]
  },
  {
   "source": [
    "### Outlier Treatment"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RemoveOutliers(data, col):\n",
    "    '''\n",
    "    Removes outliers from the specified column\n",
    "    data: DataFrame\n",
    "    col: Column to filter data\n",
    "    '''\n",
    "    u = np.median(data[col])\n",
    "    s = np.std(data[col])\n",
    "    q1 = data[col].quantile(0.25)\n",
    "    q3 = data[col].quantile(0.75)\n",
    "    iqr = q3-q1\n",
    "    \n",
    "    filtered = data.loc[(data[col]>q1-iqr)\n",
    "                        &\n",
    "                        (data[col]<q3+iqr)\n",
    "                       ]\n",
    "    return filtered\n",
    "\n",
    "\n",
    "def ImputeOutliers(data, col):\n",
    "    '''\n",
    "    Imputes outliers with the median of the specified column\n",
    "    data: DataFrame\n",
    "    col: Column to filter data\n",
    "    '''\n",
    "    u = np.median(data[col])\n",
    "    s = np.std(data[col])\n",
    "    q1 = data[col].quantile(0.25)\n",
    "    q3 = data[col].quantile(0.75)\n",
    "    iqr = q3-q1\n",
    "\n",
    "    data[col] = np.where((data[col]>q1-iqr)|(data[col]>q1-iqr),\n",
    "                         u,\n",
    "                         data[col])\n",
    "    filtered = data\n",
    "    return filtered\n",
    "\n",
    "\n",
    "def FilterData(data, col, minimum=0, maximum=float('inf')):\n",
    "    '''\n",
    "    data: DataFrame\n",
    "    col: Column to filter data\n",
    "    minimum: Defaults to 0, but any number can be input\n",
    "    maximum: Defaults to inf, but any number can be input\n",
    "    '''\n",
    "    filtered = data.loc[data[col]>=minimum]\n",
    "    filtered = filtered.loc[filtered[col]<=maximum]\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find Additional Insights, Visualize Data, and Determine Appropriate Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build and Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Machine Learning Packages\n",
    "# Preprocessing:\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import scale, StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Regression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Model Selection/Hyperparameter Tuning\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, explained_variance_score, roc_auc_score, classification_report, accuracy_score, f1_score\n",
    "from bayes_opt import BayesianOptimization"
   ]
  },
  {
   "source": [
    "## Define Variables"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine dependent and independent variables based on dataset\n",
    "y = df['churn']\n",
    "X = df.drop(['churn','area_code','phone'], axis=1)"
   ]
  },
  {
   "source": [
    "## Normalize/Standardize Data (Set-Up Pipeline Steps and Hyperparameter Space)\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing Pipeline\n",
    "numeric_features = list(X.select_dtypes(include=['number']))\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median'))])\n",
    "\n",
    "categorical_features = list(X.select_dtypes(include=['category']))\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)]\n",
    ")"
   ]
  },
  {
   "source": [
    "## Build Initial Model (Classification or Regression)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Model Types\n",
    "\n",
    "#### Classification\n",
    "    Logistic Regression - description\n",
    "\n",
    "    Random Forest Classifier - description\n",
    "\n",
    "    Gradient Boosting Classifier - description\n",
    "\n",
    "    XGBoost Classifier - description\n",
    "\n",
    "\n",
    "#### Regression\n",
    "    Linear Regression - description\n",
    "\n",
    "    Ridge Regression - description\n",
    "\n",
    "    Lasso Regression - description\n",
    "\n",
    "    Elastic Net Regression - description\n",
    "\n",
    "    Random Forest Regressor - description\n",
    "\n",
    "    Gradient Boosting Regressor - description\n",
    "\n",
    "    XGBoost Regressor - description"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Hyperparameters for Tuning\n",
    "\n",
    "#### Classification\n",
    "    Logistic Regression - description\n",
    "\n",
    "    Random Forest Classifier - description\n",
    "\n",
    "    Gradient Boosting Classifier - description\n",
    "\n",
    "    XGBoost Classifier - description\n",
    "\n",
    "\n",
    "#### Regression\n",
    "    Linear Regression - description\n",
    "\n",
    "    Ridge Regression - description\n",
    "\n",
    "    Lasso Regression - description\n",
    "\n",
    "    Elastic Net Regression - description\n",
    "\n",
    "    Random Forest Regressor - description\n",
    "\n",
    "    Gradient Boosting Regressor - description\n",
    "\n",
    "    XGBoost Regressor - description"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "numpy boolean subtract, the `-` operator, is not supported, use the bitwise_xor, the `^` operator, or the logical_xor function instead.",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-107-ae9172512765>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# Scoring Metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mrmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mmae\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean_absolute_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mroc_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aws/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m                           FutureWarning)\n\u001b[1;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aws/lib/python3.7/site-packages/sklearn/metrics/_regression.py\u001b[0m in \u001b[0;36mmean_squared_error\u001b[0;34m(y_true, y_pred, sample_weight, multioutput, squared)\u001b[0m\n\u001b[1;32m    254\u001b[0m         y_true, y_pred, multioutput)\n\u001b[1;32m    255\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m     output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n\u001b[0m\u001b[1;32m    257\u001b[0m                                weights=sample_weight)\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmultioutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: numpy boolean subtract, the `-` operator, is not supported, use the bitwise_xor, the `^` operator, or the logical_xor function instead."
     ]
    }
   ],
   "source": [
    "# Split Data into Training and Test Sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Instantiate Baseline Classification Models\n",
    "classifiers = [LogisticRegression(), RandomForestClassifier(n_estimators=100), GradientBoostingClassifier(), \n",
    "               XGBClassifier(objective='binary:logistic', eval_metric='auc')]\n",
    "\n",
    "# Fit to the training set\n",
    "for clf in classifiers:\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', clf)\n",
    "    ])\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    predictions = pipeline.predict(X_test)\n",
    "    \n",
    "    # Scoring Metrics\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, predictions))\n",
    "    mae = mean_absolute_error(y_test, predictions)\n",
    "    roc_score = roc_auc_score(y_test, predictions)\n",
    "    acc = accuracy_score(y_test, predictions)\n",
    "    class_report = classification_report(y_test, predictions)\n",
    "\n",
    "    roc_score = cross_val_score(clf, X_train, y_train, cv=5, scoring='roc_auc').mean()\n",
    "    precision = cross_val_score(clf, X_train, y_train, cv=5, scoring='precision').mean()\n",
    "    recall = cross_val_score(clf, X_train, y_train, cv=5, scoring='recall').mean()\n",
    "    f1 = cross_val_score(clf, X_train, y_train, cv=5, scoring='f1').mean()\n",
    "\n",
    "    print(pipeline.named_steps['classifier'].__class__.__name__)\n",
    "    print(' Root Mean Squared Error: {}'.format(rmse))\n",
    "    print(' Mean Absolute Error: {}'.format(mae))\n",
    "    print(' AUC Score: {}'.format(roc_score))\n",
    "    print(' Accuracy: {}'.format(acc))\n",
    "    print(' Classification Report:\\n{}'.format(class_report))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Baseline Regression Models\n",
    "regressors = [LinearRegression(), Ridge(), Lasso(), ElasticNet(), RandomForestRegressor(n_estimators=100), \n",
    "              GradientBoostingRegressor(), xgb.XGBRegressor(objective='reg:squarederror', eval_metric='mae')]\n",
    "\n",
    "# Split Data into Training and Test Sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Fit to the training set\n",
    "for reg in regressors:\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', reg)\n",
    "    ])\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    predictions = pipeline.predict(X_test)\n",
    "    \n",
    "    # Scoring Metrics\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, predictions))\n",
    "    mae = mean_absolute_error(y_test, predictions)\n",
    "    roc_score = roc_auc_score(y_test, predictions)\n",
    "    acc = accuracy_score(y_test, predictions)\n",
    "    class_report = classification_report(y_test, predictions)\n",
    "\n",
    "    print(pipeline.named_steps['classifier'].__class__.__name__)\n",
    "    print(' Root Mean Squared Error: {}'.format(rmse))\n",
    "    print(' Mean Absolute Error: {}'.format(mae))\n",
    "    print(' AUC Score: {}'.format(roc_score))\n",
    "    print(' Accuracy: {}'.format(acc))\n",
    "    print(' Classification Report:\\n{}'.format(class_report))\n",
    "    print('\\n')"
   ]
  },
  {
   "source": [
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Hyperparameter Tuning"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### RandomizedSearchCV\n",
    "#### Use RandomizedSearchCV to define a grid of hyperparameter ranges"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "classifier = RandomForestClassifier()\n",
    "regressor = RandomForestRegressor()\n",
    "\n",
    "# Create Pipeline Object\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor)\n",
    "    ('classifier', classifier),\n",
    "    ('regressor', regressor)\n",
    "])\n",
    "\n",
    "# Use RandomizedSearchCV for Initial Hyperparameter Examination\n",
    "parameters = {\n",
    "    'classifier__learning_rate': np.linspace(0,1,11),\n",
    "    'classifier__max_depth': range(2,20,2),\n",
    "    'classifier__n_estimators': range(50,500,50),\n",
    "    'classifier__min_samples_split': np.linspace(0.1,1,10),\n",
    "    'classifier__min_samples_leaf': np.linspace(0.1,0.5,9),\n",
    "    'classifier__max_features': ['log2', 'sqrt'],\n",
    "    'classifier__bootstrap': [True, False],\n",
    "}\n",
    "\n",
    "parameters = {\n",
    "    'regressor__learning_rate': np.linspace(0,1,11),\n",
    "    'regressor__max_depth': range(2,20,2),\n",
    "    'regressor__n_estimators': range(50,500,50),\n",
    "    'regressor__min_samples_split': np.linspace(0.1,1,10),\n",
    "    'regressor__min_samples_leaf': np.linspace(0.1,0.5,9),\n",
    "    'regressor__max_features': ['log2', 'sqrt'],\n",
    "    'regressor__bootstrap': [True, False],\n",
    "}\n",
    "\n",
    "model = RandomizedSearchCV(pipeline, parameters, n_iter=100, cv=3, random_state=42, n_jobs=-1)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(model.score(X_train, y_train))\n",
    "print(model.best_params_)"
   ]
  },
  {
   "source": [
    "### GridSearchCV\n",
    "#### Use GridSearchCV to determine the optimal hyperparameters based on the RandomSearchCV results"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "classifier = RandomForestClassifier()\n",
    "regressor = RandomForestRegressor()\n",
    "\n",
    "# Create Pipeline Object\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor)\n",
    "    ('classifier', classifier),\n",
    "    ('regressor', regressor)\n",
    "])\n",
    "\n",
    "# Use GridSearchCV for Hyperparameter Tuning\n",
    "parameters = {\n",
    "    'classifier__learning_rate': [],\n",
    "    'classifier__max_depth': [],\n",
    "    'classifier__n_estimators': [],\n",
    "    'classifier__min_samples_split': [],\n",
    "    'classifier__min_samples_leaf': [],\n",
    "    'classifier__max_features': [],\n",
    "    'classifier__bootstrap': [],\n",
    "}\n",
    "\n",
    "parameters = {\n",
    "    'regressor__learning_rate': [],\n",
    "    'regressor__max_depth': [],\n",
    "    'regressor__n_estimators': [],\n",
    "    'regressor__min_samples_split': [],\n",
    "    'regressor__min_samples_leaf': [],\n",
    "    'regressor__max_features': [],\n",
    "    'regressor__bootstrap': [],\n",
    "}\n",
    "\n",
    "model = GridSearchCV(pipeline, parameters, cv=3, n_jobs=-1)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(model.score(X_train, y_train))\n",
    "print(model.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the best parameters from model\n",
    "parameters = model.best_params_\n",
    "model = model.best_estimator_\n",
    "print(model)\n",
    "\n",
    "# Fit Model to Training Set\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate Model Performance\n",
    "print(model)\n",
    "\n",
    "predictions = model.predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, predictions))\n",
    "mae = mean_absolute_error(y_test, predictions)\n",
    "roc_score = roc_auc_score(y_test, predictions)\n",
    "acc = accuracy_score(y_test, predictions)\n",
    "class_report = classification_report(y_test, predictions)\n",
    "\n",
    "print(\"Root Mean Squared Error: {}\".format(rmse))\n",
    "print(\"Mean Absolute Error: {}\".format(mae))\n",
    "print(\"AUC Score: {}\".format(roc_score))\n",
    "print(\"Accuracy: {}\".format(acc))\n",
    "print(\"Classification Report:\\n{}\".format(class_report))\n",
    "print('\\n')"
   ]
  },
  {
   "source": [
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Examine Feature Importances"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine Variables (One Hot Encoded Categorical Variables and Numeric Variables)\n",
    "onehot_columns = model.named_steps['preprocessor'].named_transformers_['cat'].named_steps['onehot'].get_feature_names(input_features=categorical_features)\n",
    "\n",
    "feature_importance = pd.Series(data=model.named_steps['classifier'].feature_importances_, index = np.array(numeric_features + list(onehot_columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Examine Feature Importance/Correlation Coefficients\n",
    "columns = np.array(numeric_features + list(onehot_columns))\n",
    "\n",
    "categories = [i.split('_', 1)[0] for i in columns]\n",
    "\n",
    "importance_df = pd.DataFrame({\n",
    "    'variable': columns, \n",
    "    'importance': model.named_steps['classifier'].feature_importances_,\n",
    "    'category': categories\n",
    "})\n",
    "\n",
    "# Feature Importances DataFrame\n",
    "cat_importance_df = importance_df.groupby('category').sum().sort_values(by='importance', ascending=False)\n",
    "print(\"Feature Importances:\\n{}\".format(cat_importance_df))\n",
    "\n",
    "# Plot Feature Importances\n",
    "sns.barplot(x='importance', y=cat_importance_df.index, data=cat_importance_df)\n",
    "plt.title('Feature Importances')\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Create Machine Learning Pipeline Library"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Dump the Model into a Machine Learning Pipeline Library\n",
    "filename = 'model.sav'\n",
    "\n",
    "pickle.dump(model, open(filename, 'wb'))"
   ]
  },
  {
   "source": [
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Model to New Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Model to New Data\n",
    "new_df = pd.DataFrame()\n",
    "predicted_df = pd.DataFrame(model.predict_proba(new_df))\n",
    "\n",
    "# Concatenate New Dataframe with Predictions\n",
    "predicted_df = pd.concat([new_df.reset_index(), predicted_df], axis=1)"
   ]
  },
  {
   "source": [
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Appendix"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Bayesian Optimization for XGBoost"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "#Bayesian Optimization function for xgboost\n",
    "#specify parameters to tune as keyword arguments\n",
    "def bo_tune_xgb(max_depth, gamma, n_estimators ,learning_rate):\n",
    "    parameters = {'max_depth': int(max_depth),\n",
    "                  'gamma': gamma,\n",
    "                  'n_estimators': int(n_estimators),\n",
    "                  'learning_rate':learning_rate,\n",
    "                  'subsample': 0.8,\n",
    "                  'eta': 0.1,\n",
    "                  'eval_metric': 'auc'}\n",
    "    #Cross validating with the specified parameters in 3 folds and 20 iterations\n",
    "    cv_result = xgb.cv(parameters, data_dmatrix, num_boost_round=20, nfold=3)\n",
    "    #Return the AUC Score\n",
    "    return cv_result['test-auc-mean'].iloc[-1]\n",
    "\n",
    "#Invoking the Bayesian Optimizer with the specified parameters to tune\n",
    "xgb_bo = BayesianOptimization(bo_tune_xgb, {'max_depth': (2, 10),\n",
    "                                            'gamma': (0, 1),\n",
    "                                            'learning_rate':(0,1),\n",
    "                                            'n_estimators':(100,250)})\n",
    "\n",
    "# Perform Bayesian optimization (20 iterations with 5 steps of random exploration)\n",
    "xgb_bo.maximize(n_iter=20, init_points=5, acq='ei')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the best parameters\n",
    "parameters = xgb_bo.max['params']\n",
    "print(parameters)\n",
    "\n",
    "# Converting the max_depth and n_estimator values from float to int\n",
    "parameters['max_depth']= int(parameters['max_depth'])\n",
    "parameters['n_estimators']= int(parameters['n_estimators'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit Model to Training Set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=9999)\n",
    "\n",
    "#Initialize an XGBClassifier with the tuned parameters and fit the training data\n",
    "model = XGBClassifier(**parameters).fit(X_train, y_train)\n"
   ]
  },
  {
   "source": [
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Examine Feature Importances"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine Feature Importance/Correlation Coefficients\n",
    "categories = [i.split('__', 1)[0] for i in columns]\n",
    "\n",
    "importance_df = pd.DataFrame({'variable': columns, \n",
    "                              'importance': model.feature_importances_,\n",
    "                              'category': categories})\n",
    "\n",
    "print(importance_df.groupby('category').sum().sort_values(by='importance', ascending=False))"
   ]
  },
  {
   "source": [
    "---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# References"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "\n",
    "scaler = scaler.fit(train_df[metrics])\n",
    "\n",
    "train_df.loc[:,metrics] = scaler.transform(train_df[metrics])\n",
    "test_df.loc[:,metrics] = scaler.transform(test_df[metrics])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Set Random Seed\n",
    "random.seed(42)\n",
    "\n",
    "# Train/Test Split Thresholds\n",
    "train_pct = 0.9\n",
    "\n",
    "# Shuffle Devices\n",
    "n_devices = nn_df.index.get_level_values('device').nunique()\n",
    "shuffled = random.sample(list(nn_df.index.get_level_values('device').unique()), n_devices)\n",
    "\n",
    "# Bucket Devices into Train/Test Sets\n",
    "train_devices = shuffled[:int(n_devices*train_pct)]\n",
    "test_devices = shuffled[int(n_devices*train_pct):]\n",
    "\n",
    "# Create Train/Test Dataframes\n",
    "train_df = nn_df.loc[train_devices]\n",
    "test_df = nn_df.loc[test_devices]\n",
    "\n",
    "print(f'Devices in Training Set: {len(train_devices)}')\n",
    "print(f'Devices in Test Set: {len(test_devices)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_generator(predictors, target, time_steps=1, step=1):\n",
    "        X, y = [], []\n",
    "        for i in range(0, len(predictors) - time_steps, step):\n",
    "            v = predictors.iloc[i:(i+time_steps)].values\n",
    "            z = target.iloc[i:i+time_steps]\n",
    "            X.append(v)\n",
    "            y.append(z[0])\n",
    "        return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_steps = 5\n",
    "step = 1\n",
    "\n",
    "X_train, y_train = dataset_generator(\n",
    "    train_df[metrics],\n",
    "    train_df['failure'],\n",
    "    time_steps,\n",
    "    step\n",
    ")\n",
    "\n",
    "X_test, y_test = dataset_generator(\n",
    "    test_df[metrics],\n",
    "    test_df['failure'],\n",
    "    time_steps,\n",
    "    step\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "5099388e4bf71856098509200a1477eb9ba8acb9397831d88b00ce066abeee44"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}